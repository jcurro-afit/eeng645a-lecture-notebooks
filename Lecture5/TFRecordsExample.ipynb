{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import h5py\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TensorFlow Records (TFRecords)\n",
    "TFRecords are TensorFlow specific objects for quickly and efficiently storing data on disk and reading it back in for training. They use [Protocol Buffers](https://developers.google.com/protocol-buffers) (a google code project) to efficiently store different data types. TensorFlow introduced some convenience (sorta) functions to work with protocol buffers without directly using the base library. This is better explained with an example.\n",
    "\n",
    "So first the dataset which we will read in with Pandas (cause Pandas makes it easy to look at). The input features are some variables done to a plane simulation to alter magnetometer readings. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cosX</th>\n",
       "      <th>cosY</th>\n",
       "      <th>cosZ</th>\n",
       "      <th>flap angle(rad)</th>\n",
       "      <th>rudder angle(rad)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.364360</td>\n",
       "      <td>-0.23860</td>\n",
       "      <td>0.900170</td>\n",
       "      <td>0.783300</td>\n",
       "      <td>-0.29378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.516630</td>\n",
       "      <td>-0.56340</td>\n",
       "      <td>0.644730</td>\n",
       "      <td>0.343350</td>\n",
       "      <td>0.59530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.037998</td>\n",
       "      <td>-0.99223</td>\n",
       "      <td>-0.118450</td>\n",
       "      <td>0.588880</td>\n",
       "      <td>-0.46642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.629490</td>\n",
       "      <td>-0.18223</td>\n",
       "      <td>0.755340</td>\n",
       "      <td>-0.115410</td>\n",
       "      <td>-0.38319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.500480</td>\n",
       "      <td>0.11948</td>\n",
       "      <td>0.857470</td>\n",
       "      <td>-0.055621</td>\n",
       "      <td>0.52118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>-0.139920</td>\n",
       "      <td>0.91155</td>\n",
       "      <td>0.386650</td>\n",
       "      <td>0.226880</td>\n",
       "      <td>0.33034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>0.577470</td>\n",
       "      <td>0.73689</td>\n",
       "      <td>-0.351450</td>\n",
       "      <td>-0.191640</td>\n",
       "      <td>0.68059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>0.087447</td>\n",
       "      <td>-0.93206</td>\n",
       "      <td>-0.351580</td>\n",
       "      <td>0.181340</td>\n",
       "      <td>-0.50669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>0.206470</td>\n",
       "      <td>0.97796</td>\n",
       "      <td>0.030993</td>\n",
       "      <td>-0.669570</td>\n",
       "      <td>0.13727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>0.248740</td>\n",
       "      <td>-0.36039</td>\n",
       "      <td>0.899030</td>\n",
       "      <td>-0.456660</td>\n",
       "      <td>0.71166</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          cosX     cosY      cosZ   flap angle(rad)   rudder angle(rad)\n",
       "0    -0.364360 -0.23860  0.900170          0.783300            -0.29378\n",
       "1    -0.516630 -0.56340  0.644730          0.343350             0.59530\n",
       "2    -0.037998 -0.99223 -0.118450          0.588880            -0.46642\n",
       "3     0.629490 -0.18223  0.755340         -0.115410            -0.38319\n",
       "4    -0.500480  0.11948  0.857470         -0.055621             0.52118\n",
       "...        ...      ...       ...               ...                 ...\n",
       "9995 -0.139920  0.91155  0.386650          0.226880             0.33034\n",
       "9996  0.577470  0.73689 -0.351450         -0.191640             0.68059\n",
       "9997  0.087447 -0.93206 -0.351580          0.181340            -0.50669\n",
       "9998  0.206470  0.97796  0.030993         -0.669570             0.13727\n",
       "9999  0.248740 -0.36039  0.899030         -0.456660             0.71166\n",
       "\n",
       "[10000 rows x 5 columns]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = pd.read_csv('/opt/data/MagData/EE699_IN.txt')\n",
    "inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Out output target data is the magentic field reading of the magnetometer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Magnetic Field (nT)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-112.69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>844.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1851.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1279.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-246.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>-703.47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>-871.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>1929.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>-932.63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>-935.26</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Magnetic Field (nT)\n",
       "0                 -112.69\n",
       "1                  844.88\n",
       "2                 1851.20\n",
       "3                -1279.80\n",
       "4                 -246.01\n",
       "...                   ...\n",
       "9995              -703.47\n",
       "9996              -871.67\n",
       "9997              1929.20\n",
       "9998              -932.63\n",
       "9999              -935.26\n",
       "\n",
       "[10000 rows x 1 columns]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs = pd.read_csv('/opt/data/MagData/EE699_Out.txt')\n",
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "cosX                 -0.016558\n",
       " cosY                -0.006312\n",
       " cosZ                 0.354091\n",
       " flap angle(rad)      0.002300\n",
       " rudder angle(rad)   -0.005623\n",
       "dtype: float64"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Magnetic Field (nT)   -0.000074\n",
       "dtype: float64"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "cosX                  0.603413\n",
       " cosY                 0.598194\n",
       " cosZ                 0.390447\n",
       " flap angle(rad)      0.450745\n",
       " rudder angle(rad)    0.454901\n",
       "dtype: float64"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Magnetic Field (nT)    1172.838237\n",
       "dtype: float64"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs.std()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we would usually split the data into train, valid, test and preprocess it by normalization etc. but for right now we are only interested in making our generator so assume we do that here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((10000, 5), (10000, 1))"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we did some preprocessing by turning into numpy arrays\n",
    "inputs = inputs.to_numpy()\n",
    "outputs = outputs.to_numpy()\n",
    "\n",
    "inputs.shape, outputs.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Later we will need to know the types of these two arrays so lets print it out here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(dtype('float64'), dtype('float64'))"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs.dtype, outputs.dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TFRecord helper functions\n",
    "\n",
    "Before we get into how we make tfrecords objects and files we will define these helper functions right from the tfrecords [webpage](https://www.tensorflow.org/tutorials/load_data/tfrecord)\n",
    "\n",
    "These functions allow us to turn primitives like an `int` into `Feature` objects for the tfrecords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following functions can be used to convert a value to a type compatible\n",
    "# with tf.train.Example.\n",
    "\n",
    "def _bytes_feature(value):\n",
    "    \"\"\"Returns a bytes_list from a string / byte.\"\"\"\n",
    "    if isinstance(value, type(tf.constant(0))):\n",
    "        value = value.numpy()  # BytesList won't unpack a string from an EagerTensor.\n",
    "    return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\n",
    "\n",
    "\n",
    "def _float_feature(value):\n",
    "    \"\"\"Returns a float_list from a float / double.\"\"\"\n",
    "    return tf.train.Feature(float_list=tf.train.FloatList(value=[value]))\n",
    "\n",
    "\n",
    "def _int64_feature(value):\n",
    "    \"\"\"Returns an int64_list from a bool / enum / int / uint.\"\"\"\n",
    "    return tf.train.Feature(int64_list=tf.train.Int64List(value=[value]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Made Dataset into TFRecords\n",
    "Now we will take our data and store it in tfrecord files. The idea is we split our data into different files which can be read in parallel. For specifics on the number of files and sizes see the tfrecords [webpage](https://www.tensorflow.org/tutorials/load_data/tfrecord)\n",
    "\n",
    "Our basic gameplan will be to loop over each file we want to make, and for each file we will loop over samples writing them one at a time to the file via a writer.\n",
    "\n",
    "One thing to note is that our helper function expect a single float, int64, or some byte and not a list. Thus, if we want to store a list of values we need to make our own helper functions that work with lists or just convert everything to bytes (which is useful if the data is not 1D but ND)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make our data path if it doesn't exist\n",
    "out_path = Path('/opt/data/MagData/tfrecords/')\n",
    "if not os.path.exists(out_path):\n",
    "    os.makedirs(out_path)\n",
    "\n",
    "# set the numer of records we want\n",
    "records_per_file = 1000\n",
    "\n",
    "# the total number of samples we are going to write\n",
    "n_samples = inputs.shape[0]\n",
    "\n",
    "# how many tfrecord files we will get\n",
    "n_tfrecord_files = int(np.ceil(n_samples / records_per_file))\n",
    "\n",
    "# a list to store name of each of the files we are going to write\n",
    "record_files = [out_path / f'train{idx}.tfrecord' for idx in range(n_tfrecord_files)]\n",
    "\n",
    "# Iterate over all the files we are going to make\n",
    "for idx, record_file in enumerate(record_files):\n",
    "    # make a slice for the current samples we are going to write to the file\n",
    "    slicer = slice(idx * records_per_file, (idx + 1) * records_per_file)\n",
    "    \n",
    "    # open a tfrecordwriter to write to the record\n",
    "    with tf.io.TFRecordWriter(str(record_file)) as writer:\n",
    "        # individually loop over all the samples from the slice\n",
    "        for x_sample, y_sample in zip(inputs[slicer], outputs[slicer]):\n",
    "            # must convert any array types to bytes for storage if we do not want to store values individually\n",
    "            x_sample_bytes = x_sample.tobytes()\n",
    "            # Create a dict with the data we want to save in the\n",
    "            # TFRecords file. You can add more relevant data here.\n",
    "            data = {\n",
    "                'input': _bytes_feature(x_sample_bytes),\n",
    "                'output': _float_feature(y_sample)\n",
    "            }\n",
    "            # Wrap the data as TensorFlow Features.\n",
    "            feature = tf.train.Features(feature=data)\n",
    "            # Wrap again as a TensorFlow Example.\n",
    "            example = tf.train.Example(features=feature)\n",
    "            # Serialize the data.\n",
    "            serialized = example.SerializeToString()\n",
    "            # Write the serialized data to the TFRecords file.\n",
    "            writer.write(serialized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now to prove that we are reading from the TFRecords and not using the original data I am going to delete the original inputs and outputs variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features does not exist\n"
     ]
    }
   ],
   "source": [
    "del inputs\n",
    "del outputs\n",
    "\n",
    "try:\n",
    "    print(features)\n",
    "except NameError:\n",
    "    print(\"features does not exist\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Open our TFRecords\n",
    "To open the tfrecords we will make a TFRecordDataset object. The TFRecordDataset simply wants the file names for each tfrecord and some info on how to extract the data. However we cannot do anything with the data until we tell the dataset how to parse the data. For this we are going to apply use the `map` function to apply the `_parse_function` to each individual sample. \n",
    "\n",
    "the `_parse_function` will have to know how we stored the data in order to parse it out. For example, the parse function needs to know that our `input` feature was a vector of `float64` in order to decode the bytes into the right type. If we tried to decode into `float32` we would get an error. NOTE: the type issue is because we converted everything to bytes and have lost the original typing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<ParallelMapDataset shapes: ((None,), ()), types: (tf.float32, tf.float32)>"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def _parse_function(serialized):\n",
    "    features = {\n",
    "        'input': tf.io.FixedLenFeature([], tf.string),\n",
    "        'output': tf.io.FixedLenFeature([], tf.float32)\n",
    "    }\n",
    "    # Parse the serialized data so we get a dict with our data.\n",
    "    parsed_example = tf.io.parse_single_example(serialized=serialized, features=features)\n",
    "    # cast the example to our data type\n",
    "    \n",
    "    # the vector of x input data\n",
    "    x_sample_bytes = parsed_example['input']\n",
    "    # Decode the raw bytes so it becomes a tensor with type.\n",
    "    # we gave it float64 so thats what we need to decode\n",
    "    x_sample_float64 = tf.io.decode_raw(x_sample_bytes, tf.float64)\n",
    "    # however we usually want float32 for ANNs\n",
    "    x_sample_float32 = tf.cast(x_sample_float64, tf.float32)\n",
    "    x_sample = x_sample_float32\n",
    "    \n",
    "    # we don't need to do anything special to our y output data because it is one value\n",
    "    y_sample = parsed_example['output']\n",
    "    \n",
    "    # return our samples as a tuple\n",
    "    return x_sample, y_sample\n",
    "\n",
    "# annoyingly the TFRecordDataset function wants string not cool Path objects\n",
    "filename_str = [str(filename) for filename in record_files]\n",
    "# create a tfrecord dataset from the list of tfrecord files\n",
    "dataset = tf.data.TFRecordDataset(filenames=filename_str,\n",
    "                                  compression_type=None,\n",
    "                                  buffer_size=None,\n",
    "                                  num_parallel_reads=tf.data.experimental.AUTOTUNE)\n",
    "# Parse the serialized data in the TFRecords files.\n",
    "# This returns TensorFlow tensors for the image and labels.\n",
    "dataset = dataset.map(map_func=_parse_function,\n",
    "                      num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So our dataset objec is made but how do we acutally get anything form it?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Actual Generator\n",
    "Anyway we didn't come here to store and read data (even though that may be useful for future reference) we actually want a generator. So lets do that next.\n",
    "\n",
    "Actually making a generator out of our dataset is not necessary when using keras to train but it is *very* useful for debugging. The easiest way is to make a numpy iterator from our dataset, and use the `next()` function to get a sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([-0.36436, -0.2386 ,  0.90017,  0.7833 , -0.29378], dtype=float32),\n",
       " -112.69)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np_iterator = dataset.as_numpy_iterator()\n",
    "sample = next(np_iterator)\n",
    "sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can see our sample is a tuple of an array of numbers and a single number. This corresponds to our features and label we will use for training. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a Pipeline\n",
    "As our dataset is right now we would not be able to do any useful training. For example, we have not scaled our data or but it into batches. We only get one sample at a time. We could do this upfront in storing the data but then we could not change our scaling or batch size without rewriting the data to disk. An easier method is to chain dataset objects together with pipeline functions. \n",
    "\n",
    "For example here is how to get batches of data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[-0.36436  , -0.2386   ,  0.90017  ,  0.7833   , -0.29378  ],\n",
       "        [-0.49361  , -0.7813   , -0.382    , -0.64873  ,  0.50364  ],\n",
       "        [-0.52344  ,  0.80814  ,  0.27003  , -0.49646  ,  0.4856   ],\n",
       "        [-0.51663  , -0.5634   ,  0.64473  ,  0.34335  ,  0.5953   ],\n",
       "        [ 0.26176  , -0.91461  ,  0.30819  ,  0.36011  ,  0.16335  ],\n",
       "        [-0.39692  ,  0.91785  , -0.0011791,  0.77746  , -0.0076665],\n",
       "        [-0.037998 , -0.99223  , -0.11845  ,  0.58888  , -0.46642  ],\n",
       "        [-0.83825  ,  0.33201  ,  0.43256  , -0.26341  , -0.65432  ],\n",
       "        [ 0.68502  , -0.51613  ,  0.51416  ,  0.45831  ,  0.75734  ],\n",
       "        [ 0.62949  , -0.18223  ,  0.75534  , -0.11541  , -0.38319  ]],\n",
       "       dtype=float32),\n",
       " array([ -112.69 ,  2644.5  ,   -44.774,   844.88 ,   744.13 ,    78.066,\n",
       "         1851.2  ,   609.59 ,  -632.39 , -1279.8  ], dtype=float32))"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_batch = dataset.batch(batch_size=10)\n",
    "np_iterator = dataset_batch.as_numpy_iterator()\n",
    "sample = next(np_iterator)\n",
    "sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See how our sample is now a batch of data? That is what our training expects in order to run. An interesting property of the pipeline functions is that they do not modify the original dataset (unless you overwrite the object). So next we can use the original dataset object to do shuffling and batching:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[ 2.2899e-01, -8.3163e-01,  5.0593e-01,  6.2202e-01,  5.1776e-03],\n",
       "        [-4.1377e-01, -8.2834e-01,  3.7768e-01, -2.9729e-01, -2.5149e-01],\n",
       "        [ 7.4149e-01, -5.5486e-01, -3.7726e-01,  2.3469e-01, -2.3987e-01],\n",
       "        [-6.9353e-01, -6.2053e-01,  3.6600e-01, -5.3542e-01, -1.5070e-01],\n",
       "        [-2.3439e-01,  6.5775e-01,  7.1584e-01, -2.5533e-01, -1.4883e-04],\n",
       "        [ 8.3862e-01, -5.1312e-02,  5.4229e-01, -6.4154e-01, -7.0942e-01],\n",
       "        [ 8.9846e-01,  4.3139e-01,  8.1604e-02, -4.2677e-02, -5.0773e-01],\n",
       "        [-8.5809e-01,  3.6642e-01, -3.5976e-01, -1.7021e-01,  4.2081e-01],\n",
       "        [-5.2344e-01,  8.0814e-01,  2.7003e-01, -4.9646e-01,  4.8560e-01],\n",
       "        [ 8.6083e-01, -1.8033e-01,  4.7587e-01, -8.3081e-02,  2.6939e-01]],\n",
       "       dtype=float32),\n",
       " array([  355.17 ,  1460.3  ,   499.79 ,  1585.5  ,  -866.65 , -1382.9  ,\n",
       "        -1321.9  ,  1515.2  ,   -44.774, -1173.   ], dtype=float32))"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_batch_shuffled = dataset.shuffle(buffer_size=10*10).batch(batch_size=10)\n",
    "np_iterator = dataset_batch_shuffled.as_numpy_iterator()\n",
    "sample = next(np_iterator)\n",
    "sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that before when we kept recalling our batch creator we would always get the same samples. Now we are shuffling a buffer of samples then putting them in batches.\n",
    "\n",
    "One important point you might not realize is that our dataset is currently finite as this example will show. We are going to make batches of 1000 and try to get 11 batches will will go beyond our 10,000 samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tried to get too many samples\n"
     ]
    }
   ],
   "source": [
    "dataset_batch_shuffled = dataset.shuffle(buffer_size=10*10).batch(batch_size=1000)\n",
    "np_iterator = dataset_batch_shuffled.as_numpy_iterator()\n",
    "try:\n",
    "    for i in range(11):\n",
    "        sample = next(np_iterator)\n",
    "    else:\n",
    "        print(\"for loop ended\")\n",
    "except StopIteration:\n",
    "    print(\"tried to get too many samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To fix this we can call the repeat function which will *repeat* our data forever or a set number of times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for loop ended\n"
     ]
    }
   ],
   "source": [
    "dataset_batch_shuffled_repeated = dataset.repeat().shuffle(buffer_size=10*10).batch(batch_size=1000)\n",
    "np_iterator = dataset_batch_shuffled_repeated.as_numpy_iterator()\n",
    "try:\n",
    "    for i in range(11):\n",
    "        sample = next(np_iterator)\n",
    "    else:\n",
    "        print(\"for loop ended\")\n",
    "except StopIteration:\n",
    "    print(\"tried to get too many samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom Pipeline\n",
    "The other important function we wanted our pipeline to do was scaling. For this we can make a custom function to do our mapping for us. Note this is the same process we used to parse the data originally but with a different function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([-0.6193609 , -0.40558657,  1.5301629 ,  1.3315002 , -0.49938482],\n",
       "       dtype=float32),\n",
       " -0.11269)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# do the scaling on our image batches\n",
    "def mapper(x_sample, y_sample):\n",
    "    x_sample = tf.tensordot(x_sample, x_sample, axes=1) * x_sample\n",
    "    y_sample = y_sample / 1000\n",
    "    return x_sample, y_sample\n",
    "\n",
    "dataset_scaled = dataset.map(map_func=mapper,\n",
    "                                  num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "np_iterator = dataset_scaled.as_numpy_iterator()\n",
    "sample = next(np_iterator)\n",
    "sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is important to note that where you put the scaling matters. For example above we scaled on a per sample basis. It would probably be more efficient to do this on a batch basis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "failed\n"
     ]
    }
   ],
   "source": [
    "# do the scaling on our image batches\n",
    "def mapper(x_sample, y_sample):\n",
    "    x_sample = tf.tensordot(x_sample, x_sample, axes=1) * x_sample\n",
    "    y_sample = y_sample / 1000\n",
    "    return x_sample, y_sample\n",
    "\n",
    "from tensorflow.errors import InvalidArgumentError\n",
    "try:\n",
    "    dataset_batch = dataset.batch(batch_size=100)\n",
    "    dataset_scaled = dataset_batch.map(map_func=mapper,\n",
    "                                      num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "    np_iterator = dataset_scaled.as_numpy_iterator()\n",
    "    sample = next(np_iterator)\n",
    "    sample\n",
    "except InvalidArgumentError:\n",
    "    print('failed')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What happened? apparently we messed up and our function doesn't work on a batch of data. This will happen if we are not careful with what shapes our tensors are.\n",
    "\n",
    "For a working example tensordot was not what we needed and so we went with reduce_sum and multiply to achieve the same result over the batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[-6.19360924e-01, -4.05586571e-01,  1.53016293e+00,\n",
       "          1.33150017e+00, -4.99384820e-01],\n",
       "        [-8.26554060e-01, -1.30829334e+00, -6.39662206e-01,\n",
       "         -1.08630371e+00,  8.43349397e-01],\n",
       "        [-7.75882423e-01,  1.19788623e+00,  4.00258899e-01,\n",
       "         -7.35890567e-01,  7.19793081e-01],\n",
       "        [-7.60620952e-01, -8.29479158e-01,  9.49219227e-01,\n",
       "          5.05505264e-01,  8.76444757e-01],\n",
       "        [ 3.02692264e-01, -1.05763054e+00,  3.56382668e-01,\n",
       "          4.16421592e-01,  1.88893571e-01],\n",
       "        [-6.36857450e-01,  1.47268879e+00, -1.89186388e-03,\n",
       "          1.24743319e+00, -1.23008853e-02],\n",
       "        [-5.94411045e-02, -1.55216718e+00, -1.85293943e-01,\n",
       "          9.21197951e-01, -7.29631066e-01],\n",
       "        [-1.25529718e+00,  4.97192055e-01,  6.47767782e-01,\n",
       "         -3.94462079e-01, -9.79858100e-01],\n",
       "        [ 1.22181213e+00, -9.20577228e-01,  9.17063534e-01,\n",
       "          8.17448676e-01,  1.35080314e+00],\n",
       "        [ 7.30307877e-01, -2.11415589e-01,  8.76313746e-01,\n",
       "         -1.33893833e-01, -4.44560975e-01],\n",
       "        [-3.81566823e-01, -6.43539727e-02,  9.27604139e-01,\n",
       "         -6.83109462e-02,  2.12844796e-02],\n",
       "        [ 1.00007725e+00, -2.15451613e-01,  2.31720611e-01,\n",
       "         -1.50323123e-01, -1.76766828e-01],\n",
       "        [-6.37978256e-01,  1.52305081e-01,  1.09304512e+00,\n",
       "         -7.09019154e-02,  6.64365232e-01],\n",
       "        [-1.98811725e-01,  8.70832622e-01,  4.57961589e-01,\n",
       "         -3.88418622e-02,  4.81279716e-02],\n",
       "        [-8.04341733e-01, -7.61180699e-01,  4.27454889e-01,\n",
       "         -5.13373196e-01, -2.19437131e-03],\n",
       "        [-7.86051452e-01, -8.81275296e-01,  6.31018221e-01,\n",
       "         -7.69502461e-01, -1.24235451e-01],\n",
       "        [-6.95617437e-01, -1.74809217e-01,  7.04555452e-01,\n",
       "         -3.63291912e-02,  6.42884076e-02],\n",
       "        [ 3.82049590e-01, -9.99481916e-01, -5.78437634e-02,\n",
       "         -1.85072184e-01, -2.18966171e-01],\n",
       "        [ 1.66669592e-01, -1.88799918e+00,  2.30200484e-01,\n",
       "         -1.22437561e+00,  1.34738982e+00],\n",
       "        [ 6.75200045e-01,  2.30739936e-01,  8.04262221e-01,\n",
       "         -2.06538111e-01, -2.10279658e-01],\n",
       "        [ 1.18699767e-01,  2.49533087e-01,  1.12774646e+00,\n",
       "         -1.26072779e-01, -4.48662609e-01],\n",
       "        [ 4.92669553e-01,  1.24142356e-02,  8.94080162e-01,\n",
       "         -2.92612519e-02,  1.44672796e-01],\n",
       "        [ 1.33086789e+00,  7.70499587e-01,  9.88015473e-01,\n",
       "          1.43355727e+00,  8.43157172e-01],\n",
       "        [ 1.73902929e-01,  1.48473573e+00,  3.62788737e-01,\n",
       "         -1.03129590e+00, -4.58469987e-01],\n",
       "        [-2.52776921e-01,  1.23726499e+00,  7.36345053e-01,\n",
       "          9.88040268e-01,  1.02976047e-01],\n",
       "        [ 8.24995995e-01, -6.17347836e-01, -4.19746697e-01,\n",
       "          2.61120588e-01, -2.66883969e-01],\n",
       "        [-5.51203787e-01, -1.21615613e+00,  6.09413743e-01,\n",
       "          7.09453821e-01,  7.10084915e-01],\n",
       "        [ 9.29242671e-01, -1.94661334e-01,  5.13688803e-01,\n",
       "         -8.96836892e-02,  2.90799201e-01],\n",
       "        [ 1.42487383e+00,  3.73454064e-01,  1.27384529e-01,\n",
       "         -1.02198291e+00,  3.91092561e-02],\n",
       "        [ 1.33539820e+00, -5.14541976e-02,  6.79753780e-01,\n",
       "         -9.76472199e-01, -4.11117792e-01],\n",
       "        [-4.76507246e-01, -9.53935802e-01,  4.34945166e-01,\n",
       "         -3.42366129e-01, -2.89621800e-01],\n",
       "        [-6.13185048e-01, -1.14212954e+00, -3.50619763e-01,\n",
       "         -6.73454762e-01, -4.06041563e-01],\n",
       "        [ 4.99842614e-01, -1.22182989e+00, -4.22458619e-01,\n",
       "          8.61023366e-01, -1.81532893e-02],\n",
       "        [-6.97296739e-01, -1.28300858e+00,  9.44167614e-01,\n",
       "         -6.98705196e-01,  1.32136869e+00],\n",
       "        [ 6.42789721e-01,  1.05553234e+00,  1.09132636e+00,\n",
       "         -3.04239869e-01,  1.29261947e+00],\n",
       "        [ 1.81033283e-01, -1.69925725e+00,  8.11143696e-01,\n",
       "          1.12745965e+00,  1.38536263e+00],\n",
       "        [-3.93290162e-01, -3.82821620e-01,  1.39576530e+00,\n",
       "          1.03718030e+00, -2.20124289e-01],\n",
       "        [-8.65102053e-01, -2.68273413e-01,  4.35571134e-01,\n",
       "          9.35725775e-03,  7.06578270e-02],\n",
       "        [ 1.97014320e+00, -1.32723168e-01, -1.41273467e-02,\n",
       "         -1.21163476e+00, -1.52724552e+00],\n",
       "        [ 1.50987804e+00, -4.14095163e-01,  1.75680071e-01,\n",
       "         -3.89281571e-01,  1.12997055e+00],\n",
       "        [ 3.17596704e-01, -1.15342569e+00,  7.01697469e-01,\n",
       "          8.62707973e-01,  7.18104979e-03],\n",
       "        [ 4.99825418e-01,  1.43018198e+00,  5.02842367e-01,\n",
       "         -1.07169068e+00, -6.08962655e-01],\n",
       "        [-1.53134930e+00, -9.58472416e-02,  2.16723174e-01,\n",
       "          8.98085594e-01,  7.16274261e-01],\n",
       "        [ 7.21947730e-01,  1.36275983e+00,  3.23549688e-01,\n",
       "         -5.33125043e-01, -1.07022095e+00],\n",
       "        [-6.08993590e-01, -1.20610273e+00, -4.25330848e-01,\n",
       "         -1.38701752e-01,  9.03567791e-01],\n",
       "        [-1.63467383e+00, -9.98293638e-01, -3.80406469e-01,\n",
       "          1.31962764e+00,  1.37551689e+00],\n",
       "        [-7.39460170e-01, -2.37246454e-01,  6.69256032e-01,\n",
       "          1.27152711e-01, -1.01439215e-01],\n",
       "        [-9.83959317e-01, -4.83519971e-01,  8.45626891e-01,\n",
       "          8.46097589e-01, -1.46086097e-01],\n",
       "        [-2.49670833e-01,  7.00631380e-01,  7.62508452e-01,\n",
       "         -2.71975994e-01, -1.58532828e-04],\n",
       "        [-2.69519925e-01, -1.70712709e+00, -6.94876730e-01,\n",
       "          1.44164944e+00, -9.56666291e-01],\n",
       "        [ 8.64602029e-01, -3.57344955e-01,  4.00172651e-01,\n",
       "          1.26458138e-01,  4.62943837e-02],\n",
       "        [ 6.75277412e-01, -1.27891374e+00, -4.08625990e-01,\n",
       "         -5.40200889e-01, -9.18634534e-01],\n",
       "        [ 1.73793066e+00, -9.23184335e-01, -1.77642152e-01,\n",
       "          1.34628570e+00, -1.41338754e+00],\n",
       "        [-8.61594174e-03,  1.71584320e+00,  3.12974676e-02,\n",
       "          1.27254391e+00, -6.99881136e-01],\n",
       "        [-3.93661320e-01, -7.87282169e-01,  4.96796757e-01,\n",
       "          4.14725058e-02,  9.61482525e-02],\n",
       "        [ 8.07999432e-01, -6.86818838e-01,  6.56006217e-01,\n",
       "          8.17788169e-02, -6.14282548e-01],\n",
       "        [-8.96197617e-01, -1.90594755e-02,  4.77939546e-01,\n",
       "          1.14740841e-01,  5.65567240e-02],\n",
       "        [-4.92432415e-01, -8.52313280e-01,  4.25850958e-01,\n",
       "          1.66357085e-01,  2.36080974e-01],\n",
       "        [-7.73378968e-01,  8.18406761e-01,  2.83333480e-01,\n",
       "          4.25766587e-01, -1.89504385e-01],\n",
       "        [ 7.28116572e-01, -2.62612011e-02,  8.18451822e-01,\n",
       "         -3.17455232e-01, -1.19230628e-01],\n",
       "        [-6.12001479e-01, -7.77187169e-01,  4.04904753e-01,\n",
       "          1.08513325e-01,  2.58713186e-01],\n",
       "        [ 8.99506807e-01, -9.59487498e-01,  1.08193541e+00,\n",
       "          1.09043360e+00, -9.21935737e-01],\n",
       "        [ 6.54891729e-01,  9.90242124e-01,  5.27212858e-01,\n",
       "          6.25378430e-01,  3.36792231e-01],\n",
       "        [ 1.49228072e+00,  9.87727642e-02,  4.72775429e-01,\n",
       "         -9.26023126e-01,  7.35576570e-01],\n",
       "        [-1.26305807e+00, -2.30074912e-01,  8.38716090e-01,\n",
       "          1.04017544e+00,  4.15616214e-01],\n",
       "        [-5.16991675e-01,  8.97989333e-01,  1.79499030e-01,\n",
       "         -3.68862152e-02,  2.36022964e-01],\n",
       "        [ 4.94507641e-01,  1.15062654e+00, -1.62470073e-01,\n",
       "          1.32943839e-01,  6.33715391e-01],\n",
       "        [ 1.05611391e-01, -5.40015101e-01,  1.25525665e+00,\n",
       "         -4.72514927e-01, -6.87610924e-01],\n",
       "        [ 4.16126639e-01,  6.91427104e-03,  9.65791583e-01,\n",
       "          2.08773196e-01,  1.16343834e-01],\n",
       "        [ 1.13169825e+00,  5.43377936e-01,  1.02788225e-01,\n",
       "         -5.37558608e-02, -6.39535666e-01],\n",
       "        [-1.35116172e+00,  1.92161947e-01,  5.66039979e-01,\n",
       "          5.48457921e-01,  8.61123681e-01],\n",
       "        [-6.76139235e-01, -1.00784707e+00,  5.04751563e-01,\n",
       "         -5.95354795e-01,  4.34508860e-01],\n",
       "        [ 4.32451010e-01, -1.01173449e+00, -3.09322983e-01,\n",
       "         -4.13444072e-01,  1.25562474e-01],\n",
       "        [-1.03490961e+00,  4.41925168e-01, -4.33892787e-01,\n",
       "         -2.05283791e-01,  5.07522881e-01],\n",
       "        [ 9.22975898e-01,  1.51629734e+00,  8.62266421e-02,\n",
       "         -1.27852440e+00, -9.05630410e-01],\n",
       "        [ 1.32742345e+00, -1.04738891e+00,  9.94057298e-01,\n",
       "          1.49104667e+00, -1.21477795e+00],\n",
       "        [-7.32585371e-01, -8.94303620e-01,  3.20941031e-01,\n",
       "          5.32786071e-01, -6.10171854e-02],\n",
       "        [ 1.60582757e+00, -9.82545391e-02,  1.03840137e+00,\n",
       "         -1.22844982e+00, -1.35842955e+00],\n",
       "        [ 8.50289226e-01, -1.01512873e+00, -5.63611746e-01,\n",
       "         -6.48809075e-01,  6.98977649e-01],\n",
       "        [ 1.89727679e-01,  6.09607458e-01,  1.44628429e+00,\n",
       "         -1.18865550e+00, -1.97584912e-01],\n",
       "        [ 6.67561173e-01,  1.43110883e+00,  3.92391115e-01,\n",
       "          2.64447063e-01,  1.26118398e+00],\n",
       "        [-6.79013789e-01,  9.00833368e-01, -5.04197299e-01,\n",
       "         -4.85316962e-01, -3.52437705e-01],\n",
       "        [-9.30470288e-01, -3.24445277e-01,  1.56104803e+00,\n",
       "         -1.20344734e+00,  1.19794607e+00],\n",
       "        [ 6.88718379e-01, -1.15234137e+00, -3.50978017e-01,\n",
       "          2.95113474e-01,  8.11909020e-01],\n",
       "        [ 3.76902819e-01,  1.36252749e+00,  8.66473615e-01,\n",
       "          1.17497981e+00,  6.54767334e-01],\n",
       "        [ 6.59082413e-01,  1.03161550e+00,  2.90500045e-01,\n",
       "         -5.08415937e-01, -3.87555659e-01],\n",
       "        [-9.94848907e-01,  1.89544216e-01,  5.93411505e-01,\n",
       "         -1.65669248e-01,  4.60444093e-01],\n",
       "        [-1.19406894e-01,  1.57437146e+00, -2.72377074e-01,\n",
       "          6.11758888e-01,  1.08245850e+00],\n",
       "        [ 2.83876598e-01,  5.96284330e-01,  9.05900657e-01,\n",
       "         -3.86431813e-01, -5.31127937e-02],\n",
       "        [ 1.00569141e+00,  7.29506789e-03,  8.85393262e-01,\n",
       "          4.10431296e-01,  6.64708197e-01],\n",
       "        [-5.49189091e-01,  1.45045757e+00, -2.62341052e-01,\n",
       "          2.88012028e-02,  1.19031870e+00],\n",
       "        [-1.18077040e-01,  1.25909233e+00,  6.25729561e-01,\n",
       "         -4.86849427e-01,  7.62295663e-01],\n",
       "        [-1.45309055e+00,  2.62390912e-01, -2.02001929e-01,\n",
       "         -6.66499138e-01,  8.03074718e-01],\n",
       "        [-2.84417421e-01, -1.37750769e+00, -4.91461456e-01,\n",
       "         -1.05058178e-01, -1.03761923e+00],\n",
       "        [-9.85503972e-01, -7.01542735e-01,  5.35287976e-01,\n",
       "         -3.59072208e-01, -6.60309732e-01],\n",
       "        [ 1.33594441e+00,  2.84107685e-01,  1.18516541e+00,\n",
       "         -1.20200086e+00,  1.09476662e+00],\n",
       "        [ 2.05875874e-01,  3.13663632e-01,  1.32980609e+00,\n",
       "         -4.15565282e-01,  7.45685160e-01],\n",
       "        [ 7.52511144e-01,  8.46804500e-01,  2.14681067e-02,\n",
       "          3.53061825e-01, -2.14919016e-01],\n",
       "        [ 3.54731679e-01,  9.02301192e-01,  7.99028873e-01,\n",
       "         -5.21701217e-01,  3.63953292e-01],\n",
       "        [-7.70653844e-01,  8.27991903e-01,  3.77778023e-01,\n",
       "          3.99530232e-01,  3.37970465e-01]], dtype=float32),\n",
       " array([-0.11269   ,  2.6445    , -0.044774  ,  0.84488   ,  0.74413   ,\n",
       "         0.078066  ,  1.8512    ,  0.60959005, -0.63239   , -1.2798    ,\n",
       "        -0.35645002, -0.89764   , -0.24600999, -0.70971   ,  1.61      ,\n",
       "         1.3011999 ,  0.55579   ,  1.2161    ,  1.4006    , -1.6529    ,\n",
       "        -1.3168    , -1.4765999 , -1.5809001 , -1.0091    , -0.67024   ,\n",
       "         0.49979   ,  1.3365    , -1.173     , -1.2728001 , -1.355     ,\n",
       "         1.4603001 ,  2.5224    ,  1.354     ,  1.0288    , -1.598     ,\n",
       "         0.73253995, -0.2859    ,  1.3206999 , -0.8021    , -0.72308004,\n",
       "         0.35517   , -1.2996999 ,  1.6941    , -1.3096999 ,  2.5145    ,\n",
       "         2.4077    ,  0.74546003,  0.93827003, -0.86665004,  2.1597    ,\n",
       "        -0.83669   ,  1.2793001 , -0.10933   , -0.53679997,  1.1805999 ,\n",
       "        -0.52808005,  1.0323    ,  1.4556999 ,  0.34429002, -1.4966999 ,\n",
       "         1.5595    , -0.53022   , -1.4257    , -1.3421999 ,  1.0014    ,\n",
       "         0.013941  , -0.95213   , -0.66646004, -1.3975999 , -1.3219    ,\n",
       "         1.0744001 ,  1.5056    ,  1.4108001 ,  1.5151999 , -1.233     ,\n",
       "        -0.55625   ,  1.7929    , -1.3829    ,  0.99207   , -1.5013001 ,\n",
       "        -1.2701    ,  0.69023997, -0.038688  ,  1.0802    , -1.2046    ,\n",
       "        -1.3969    ,  0.73371994, -0.33457997, -1.5561999 , -1.4585    ,\n",
       "         0.064247  , -0.90769   ,  1.7079    ,  2.3088    ,  1.4838    ,\n",
       "        -1.6881    , -1.4233    , -1.2522    , -1.5107    ,  0.2549    ],\n",
       "       dtype=float32))"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# do the scaling on our image batches\n",
    "def mapper(x_sample, y_sample):\n",
    "    x_sample = tf.math.reduce_sum(x_sample * x_sample, axis=1)[:, None] * x_sample\n",
    "    y_sample = y_sample / 1000\n",
    "    return x_sample, y_sample\n",
    "\n",
    "dataset_batch = dataset.batch(batch_size=100)\n",
    "dataset_scaled = dataset_batch.map(map_func=mapper,\n",
    "                                  num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "np_iterator = dataset_scaled.as_numpy_iterator()\n",
    "sample_batched = next(np_iterator)\n",
    "sample_batched"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To verify we did this correctly lets examine the first sample of the batch compared to our original sample. Remember our samples are tuples of the input and output so lets compare the inputs (using the zero index) since that is what gave us trouble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([-0.6193609 , -0.40558657,  1.5301629 ,  1.3315002 , -0.49938482],\n",
       "       dtype=float32),\n",
       " array([-0.6193609 , -0.40558657,  1.5301629 ,  1.3315002 , -0.49938482],\n",
       "       dtype=float32))"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample[0], sample_batched[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparing numbers manually is annoying lets use the `allclose` function from numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.allclose(sample[0], sample_batched[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
